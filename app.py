# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gjC47FFn7tLLMOgKmYmwd4PNYy9YzEdS
"""

import gradio as gr

from src.llm_setup import get_model, get_blip_captioner
from src.profile_store import create_profile
from src.image_pipeline import analyze_image
from src.pdf_upload import upload_and_index

import gradio as gr
import numpy as np
import faiss
import os
import json
from datetime import datetime
import torch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    CSVLoader
)

from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from PIL import Image

from transformers import (
    CLIPProcessor,
    CLIPModel,
    BlipProcessor,
    BlipForConditionalGeneration
)

llm = get_model()


def cb_pdf_upload(files, state):
    state = state or init_state()

    if not state.get("user_profile"):
        return "‚ö†Ô∏è Please create your profile first.", state

    if not files:
        return "‚ùå No files received.", state

    try:
        file_paths = [f.name if hasattr(f, "name") else f for f in files]
        documents = load_documents(file_paths)
        if not documents:
            return "‚ùå No readable text found in uploaded files.", state

        # Raw text (careful: can be huge)
        raw_text = "\n\n".join(d.page_content for d in documents)

        # Keep a bounded preview for prompt safety
        MAX_CHARS = 12000  # tune
        raw_preview = raw_text[:MAX_CHARS]

        state["pdf_uploaded"] = True
        state["pdf_file_paths"] = file_paths
        state["pdf_raw_preview"] = raw_preview

        # Optional: a more structured plan preview (first few docs/pages)
        state["plan_summary"] = "\n\n".join(d.page_content for d in documents[:5])[:8000]

        return f"‚úÖ Loaded {len(files)} file(s). PDF text ready for chat.", state

    except Exception as e:
        return f"‚ùå PDF upload error: {e}", state

def cb_pdf_chat(message, history, state):
    history = history or []
    state = state or {}

    if not message or not message.strip():
        history.append(("", "Please type a question."))
        return history, "", state

    if not state.get("pdf_uploaded"):
        history.append((message, "‚ö†Ô∏è Upload your PDF/DOCX/CSV first."))
        return history, "", state

    profile = state.get("user_profile", {})

    vs_path = state.get("document_vectorstore_path")
    if not vs_path or not os.path.exists(vs_path):
        history.append((message, "‚ö†Ô∏è Document vectorstore not found. Re-upload the PDF."))
        return history, "", state

    # üîπ Load embeddings + vectorstore
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    vs = FAISS.load_local(
        vs_path,
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )

    # üîπ EXPLICIT: convert query ‚Üí embedding
    query_embedding = embeddings.embed_query(message)

    # üîπ Vector similarity search (top-4)
    docs = vs.similarity_search_by_vector(query_embedding, k=4)

    retrieved = "\n\n".join(d.page_content for d in docs)

    if not retrieved.strip():
        retrieved = "No relevant sections found in the uploaded document."

    prompt = f"""
You are a smart fitness assistant helping the user understand and modify their uploaded workout/diet plan.
Avoid LaTeX formatting.

User profile:
- Age: {profile.get("age")}
- Height: {profile.get("height")}
- Weight: {profile.get("weight")}
- Fitness Level: {profile.get("fitness_level")}
- Gender: {profile.get("gender")}
- Health issues: {profile.get("health_issues")}

Retrieved plan context:
{retrieved}

User question:
{message}

Rules:
- Answer using ONLY the retrieved context + the user profile.
- If the retrieved context does not contain the answer, clearly say what information is missing.
- If the user asks for changes, modify only the relevant section.
- Respect health issues and suggest safe alternatives.
"""

    try:
        reply = llm.invoke(prompt).content.strip()
    except Exception as e:
        reply = f"‚ùå PDF chat error: {e}"

    history.append((message, reply))
    return history, "", state


def init_state():
    return {
        "user_profile": None,
        "profile_path": None,
        "image_state": None,
        "document_vectorstore_path": None,
        "document_embedding_kind": None,
        "plan_summary": "",
        "pdf_meta": None,
        "pdf_uploaded": False,
    }


def cb_create_profile(age, height, weight, fitness_level, gender, health_issues, state):
    state = state or init_state()

    # ‚úÖ use the saving function
    profile, path = create_profile(
        age=int(age) if age is not None else 0,
        height=float(height) if height is not None else 0.0,
        weight=float(weight) if weight is not None else 0.0,
        fitness_level=fitness_level,
        gender=gender,
        health_issues=health_issues
    )

    state["user_profile"] = profile
    state["profile_path"] = path

    return f"‚úÖ Profile created and saved.\nüìÅ {path}\nNow choose a mode below.", state



def toggle_mode(mode):
    return (
        gr.update(visible=(mode == "Image Analyzer")),
        gr.update(visible=(mode == "PDF Reader")),
    )


# analyse image
def cb_analyze(img, state):
    state = state or init_state()
    llm = get_model()
    blip_processor, blip_model = get_blip_captioner()
    profile = state.get("user_profile")
    if not profile:
        return "‚ö†Ô∏è Create profile first.", state

    # document_vectorstore optional (load from state if present)
    document_vectorstore = None
    if state.get("document_vectorstore_path"):
        document_vectorstore = load_document_vectorstore(state)

    result = analyze_image(
        img=img,
        image_type="Body",
        user_profile=profile,
        llm=llm,
        get_image_embedding=get_image_embedding,   # your CLIP function/closure
        blip_processor=blip_processor,
        blip_model=blip_model,
        document_vectorstore=document_vectorstore, # optional
        save_dir="stores/image"
    )

    if result.get("error"):
        return result["error"], state

    state["image_state"] = result
    return result["analysis_text"], state


# image chat
def cb_image_chat(message, history, state):
    """
    Interactive chat over image analysis using:
    - analysis_text (high-level)
    - vectorstore retrieval (fine-grained)
    """
    history = history or []
    state = state or {}

    image_state = state.get("image_state")
    if not image_state or image_state.get("error"):
        history.append((message, "‚ö†Ô∏è Analyze an image first."))
        return history, "", state

    vs_path = image_state.get("profile_vectorstore_path")
    if not vs_path or not os.path.exists(vs_path):
        history.append((message, "‚ö†Ô∏è Image vectorstore not found. Re-run image analysis."))
        return history, "", state

    # Load the vectorstore
    text_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vs = FAISS.load_local(
        vs_path,
        embeddings=text_embeddings,
        allow_dangerous_deserialization=True
    )

    # Retrieve context
    docs = vs.similarity_search(message, k=4)
    retrieved = "\n\n".join([d.page_content for d in docs])

    # Also use the overall analysis_text (stored in state)
    analysis_text = image_state.get("analysis_text", "")

    prompt = f"""
You are a smart fitness assistant. You have two information sources:
1) Full analysis summary (high-level):
{analysis_text}

2) Retrieved detail from vector database (most relevant chunks):
{retrieved}

User question:
{message}

Rules:
- Use the retrieved chunks first if they directly answer the question.
- Use the full analysis summary to maintain consistency and context.
- If the user asks for changes to workouts/diet, give specific actionable steps.
- If a safety/medical issue is mentioned, suggest safer alternatives.
- Please avoid LaTeX formatting like \\text{{...}} and instead use plain text for calculations.
"""

    reply = llm.invoke(prompt).content.strip()

    history.append((message, reply))
    return history, "", state

with gr.Blocks() as demo:
    gr.Markdown("# üèãÔ∏è Smart Gym RAG (Image Analyzer Lite)")
    gr.Markdown("Free HF Space: caption-based image analysis + PDF reader + chat memory via FAISS.")

    state = gr.State(init_state())

    # ------------------ STEP 1: PROFILE (VISIBLE INITIALLY) ------------------ #
    gr.Markdown("## 1) Create Profile")
    with gr.Row():
        age = gr.Number(label="Age", value=25)
        height = gr.Number(label="Height (cm)", value=160)
        weight = gr.Number(label="Weight (kg)", value=65)

    with gr.Row():
        fitness_level = gr.Dropdown(["beginner", "intermediate", "advanced"], value="beginner", label="Fitness level")
        gender = gr.Radio(["male", "female"], value="female", label="Gender")
        health_issues = gr.Textbox(label="Health issues", value="None")

    btn_profile = gr.Button("Create Profile")
    profile_status = gr.Textbox(label="Status", lines=2)

    btn_profile.click(
        cb_create_profile,
        inputs=[age, height, weight, fitness_level, gender, health_issues, state],
        outputs=[profile_status, state],
    )

    # ------------------ STEP 2: MODE SELECTOR (HIDDEN INITIALLY) ------------------ #
    mode_selector = gr.Dropdown(
        choices=["Image Analyzer", "PDF Reader"],
        label="Choose Mode",
        visible=False
    )

    # Show mode selector after profile created
    def show_mode_selector(status_text, state):
        # status_text is unused; just use it to chain
        return gr.update(visible=True)

    btn_profile.click(
        show_mode_selector,
        inputs=[profile_status, state],
        outputs=[mode_selector],
    )

    # ------------------ IMAGE ANALYZER SECTION (HIDDEN INITIALLY) ------------------ #
    with gr.Column(visible=False) as image_section:
        gr.Markdown("## 2A) Upload Image + Analyze")
        img = gr.Image(type="pil", label="Upload image")
        btn_analyze = gr.Button("Analyze Image")
        analysis_out = gr.Textbox(label="Image Analysis Output", lines=12)

        image_chatbot = gr.Chatbot(label="Chat about your Image Analysis")
        image_msg = gr.Textbox(label="Ask a question about your body / plan")
        btn_image_send = gr.Button("Send")

        btn_analyze.click(
            cb_analyze,
            inputs=[img, state],
            outputs=[analysis_out, state],
        )

        btn_image_send.click(
            cb_image_chat,
            inputs=[image_msg, image_chatbot, state],
            outputs=[image_chatbot, image_msg, state],
        )

    # ------------------ PDF READER SECTION (HIDDEN INITIALLY) ------------------ #
    with gr.Column(visible=False) as pdf_section:
        gr.Markdown("## 2B) Upload PDF/DOCX/CSV + Chat (PDF Reader)")

        pdf_files = gr.File(
            file_types=[".pdf", ".docx", ".csv"],
            file_count="multiple",
            label="Upload your workout/diet plan files"
        )
        btn_pdf_process = gr.Button("Process Files")
        pdf_status = gr.Textbox(label="PDF Upload Status", lines=2)

        pdf_chatbot = gr.Chatbot(label="Chat about your PDF Plan")
        pdf_msg = gr.Textbox(label="Ask a question about your uploaded plan")
        btn_pdf_send = gr.Button("Send (PDF Chat)")

        btn_pdf_process.click(
            cb_pdf_upload,
            inputs=[pdf_files, state],
            outputs=[pdf_status, state],
        )

        btn_pdf_send.click(
            cb_pdf_chat,
            inputs=[pdf_msg, pdf_chatbot, state],
            outputs=[pdf_chatbot, pdf_msg, state],
        )

    # ------------------ MODE TOGGLE WIRING ------------------ #
    mode_selector.change(
        toggle_mode,
        inputs=[mode_selector],
        outputs=[image_section, pdf_section]
    )

demo.launch(share=True)