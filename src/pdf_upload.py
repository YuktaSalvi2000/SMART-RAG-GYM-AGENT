# -*- coding: utf-8 -*-
"""pdf_upload.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJFl1goiTSe-O7NE3T56__o5kK6yiEVo
"""

# -------------------- standard library --------------------
import os
from datetime import datetime

# -------------------- LangChain loaders --------------------
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    CSVLoader,
)

# -------------------- LangChain text splitting --------------------
from langchain_text_splitters import RecursiveCharacterTextSplitter

# -------------------- LangChain embeddings --------------------
from langchain_community.embeddings import HuggingFaceEmbeddings

# -------------------- LangChain vectorstore (IMPORTANT: community) --------------------
from langchain_community.vectorstores import FAISS

def upload_and_index(files, state, save_dir="stores/pdf"):
    # Builds + saves document FAISS vectorstore, then stores everything needed in state.

    state = state or {}

    if not files:
        return "‚ùå No files received.", state

    os.makedirs(save_dir, exist_ok=True)

    try:
        file_paths = [f.name if hasattr(f, "name") else f for f in files]
        print("üóÇ Files received:", file_paths)

        documents = load_documents(file_paths)
        if not documents:
            return "‚ùå No readable text found in the uploaded files.", state

        chunks = chunk_documents(documents)

        # ---- keep prompt-safe previews ----
        plan_summary = "\n\n".join([doc.page_content for doc in documents[:5] if getattr(doc, "page_content", None)])
        plan_summary = plan_summary[:8000]  # cap

        raw_text = "\n\n".join([doc.page_content for doc in documents if getattr(doc, "page_content", None)])
        pdf_raw_preview = raw_text[:15000]  # cap

        doc_embeddings_local = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

        # ---- build + save vectorstore ----
        vs = FAISS.from_documents(documents=chunks, embedding=doc_embeddings_local)

        vs_path = os.path.join(save_dir, "faiss_index")
        vs.save_local(vs_path)

        # ---- store everything needed for chat/compare ----
        state["document_vectorstore_path"] = vs_path
        state["document_embedding_kind"] = embedding_kind
        state["plan_summary"] = plan_summary
        state["pdf_raw_preview"] = pdf_raw_preview  # ‚úÖ added (for prompt-based chat)
        state["pdf_uploaded"] = True
        state["pdf_meta"] = {
            "n_files": len(files),
            "n_docs": len(documents),
            "n_chunks": len(chunks),
            "created_at": str(datetime.now()),
            "file_paths": file_paths,
        }

        return f"‚úÖ Uploaded {len(files)} file(s); created {len(chunks)} chunks.", state

    except Exception as e:
        return f"‚ùå Error during file processing: {e}", state

def load_documents(file_paths):
    docs = []

    for path in file_paths:
        if path.endswith(".pdf"):
            loader = PyPDFLoader(path)
        elif path.endswith(".docx"):
            loader = Docx2txtLoader(path)
        elif path.endswith(".csv"):
            loader = CSVLoader(path)
        else:
            continue
        docs.extend(loader.load())
    return docs


def chunk_documents(documents, chunk_size=800, overlap=100):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ".", " ", "Day", "DAY"]
    )
    chunks = splitter.split_documents(documents)
    return chunks

# FUTURE EXTENSION
# # Simple Retrieval
# def query_pdf(message, state, llm, k=4):
#     vs = load_document_vectorstore(state)
#     if vs is None:
#         return "‚ö†Ô∏è Please upload and process files first."

#     docs = vs.similarity_search(message, k=k)
#     context = "\n\n".join(d.page_content for d in docs)

#     plan_summary = (state or {}).get("plan_summary", "")
#     profile = (state or {}).get("user_profile", {})

#     prompt = f"""
# You are a fitness assistant answering questions about the uploaded plan.
# Avoid LaTeX. Use profile if relevant.

# User profile:
# - Age: {profile.get("age")}
# - Height: {profile.get("height")}
# - Weight: {profile.get("weight")}
# - Fitness Level: {profile.get("fitness_level")}
# - Gender: {profile.get("gender")}
# - Health issues: {profile.get("health_issues")}

# Plan preview:
# {plan_summary}

# Relevant retrieved parts:
# {context}

# User question:
# {message}
# """
#     return llm.invoke(prompt).content.strip()