# -*- coding: utf-8 -*-
"""image_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jBDyQTy-fKxP2Jbjg2KTeHg8nm2jofrg
"""

# standard library
import os
import json
from datetime import datetime

# numerical + vector search
import numpy as np
import faiss

# torch (for CLIP embedding function)
import torch

# Hugging Face models
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    CLIPModel,
    CLIPProcessor,
)

# LangChain (community + embeddings)
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

def get_image_embedding(img):
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    clip_model.eval()

    inputs = clip_processor(images=img, return_tensors="pt")
    with torch.no_grad():
        vec = clip_model.get_image_features(**inputs)  # (1, 512)
        vec = vec / vec.norm(dim=-1, keepdim=True)

    return vec[0].detach().cpu().numpy().astype("float32")

def analyze_image(
    img,
    image_type,
    user_profile,
    llm,
    get_image_embedding,
    blip_processor, blip_model,
    create_conversation_chain=None,
    document_vectorstore=None,
    save_dir="stores/image"
):

    if not user_profile:
        return {"error": "‚ö†Ô∏è User profile is required for body analysis."}

    if img is None:
        return {"error": "‚ö†Ô∏è Please upload an image."}

    os.makedirs(save_dir, exist_ok=True)

    # Use OpenAI text embeddings for analysis text store
    text_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    images = img if isinstance(img, list) else [img]

    try:
        image_vectors = []
        image_analysis_texts = []
        clip_meta = []

        for idx, single_img in enumerate(images):
            if single_img is None:
                continue

            # -- Step 1: Generate image embedding --
            vec = get_image_embedding(single_img)
            vec = np.asarray(vec, dtype="float32")
            image_vectors.append(vec)

            # -- Step 2: Generate caption --
            inputs = blip_processor(images=single_img, return_tensors="pt")
            out = blip_model.generate(**inputs)
            caption = blip_processor.decode(out[0], skip_special_tokens=True)

            # -- Step 3: Build prompt for LLM --
            age = user_profile.get("age", "unknown")
            weight = user_profile.get("weight", "unknown")
            height = user_profile.get("height", "unknown")
            health_issues = user_profile.get("health_issues", "none")
            gender = user_profile.get("gender", "unknown")
            fitness_level = user_profile.get("fitness_level", "beginner")

            prompt = f"""
You are a certified fitness coach. A user uploads a body image which appears as: \"{caption}\".  Please avoid LaTeX formatting like \\text{{...}} and instead use plain text for calculations.

User Profile:
- Age: {age}
- Height: {height}
- Weight: {weight}
- Health Concerns: {health_issues}
- Gender: {gender}
- Fitness Level: {fitness_level}

Step into the role of a coach who designs personalized fitness plans with empathy and realism.

Your task:

1. Provide a **neutral and supportive observation** based on both the image and profile. Be kind and encouraging while evaluating the user.
   - Apparent body type (e.g., endomorph, mesomorph, ectomorph)
   - Visible fat distribution (e.g., thighs, arms, abdomen) here fat distribution patterns (e.g., belly fat, thigh fat, arm fat), rather than generalizing from body types.
   - Muscle tone visibility or lack thereof
   - Posture or alignment issues (if any)

2. Based on this body analysis, suggest which **tier of workout** is most appropriate for them. Then design all 3 tiers of progression:
   - Beginner: For low stamina or mobility
   - Intermediate: For building strength and improving endurance
   - Advanced: For refined sculpting and conditioning

   For each tier:
   - Focus primarily on **gym workouts using machines**
     - Include machine names and slot range (if needed)
   - Provide clear instructions: sets, reps, and form cues
   - Mention when to graduate to the next level

   Additionally:
   - For users preferring home-based or indoor fitness, provide **optional alternatives** like:
     - Floor-based exercises (e.g., planks, push-ups)
     - Sports activities (e.g., swimming, dance, cycling)
     - Bodyweight-based home routines

   - Suggest **weekly workout duration (hours)** and a **target calorie burn range** based on their fitness level and body goals.

3. Offer personalized coaching tips:
   - Recommend a **nutrition strategy**: whether to focus more on calories or macros
   - Clarify the **difference between fat loss and muscle visibility**
   - End with a **motivational note** reinforcing consistency, progressive overload, and celebrating effort.

Important: Respect medical conditions (e.g., if the user has knee pain, suggest alternatives to lunges or jumping).
"""

            # -- Step 4: Get analysis from LLM --
            result = llm.invoke(prompt).content.strip()
            image_analysis_texts.append({"caption": caption, "analysis": result})

            clip_meta.append({
                "idx": idx,
                "caption": caption,
                "user_profile": user_profile,
                "created_at": str(datetime.now())
            })

        if not image_analysis_texts:
            return {"error": "‚ö†Ô∏è Could not analyze the image(s). Please try again with a clearer image."}

        # -- Step 5: Save CLIP vectors properly (raw FAISS + meta JSON) --
        clip_index_path, clip_meta_path = None, None
        if image_vectors:
            vectors_np = np.vstack(image_vectors).astype("float32")
            faiss.normalize_L2(vectors_np)
            dim = vectors_np.shape[1]

            clip_index = faiss.IndexFlatIP(vectors_np.shape[1])
            clip_index.add(vectors_np)

            clip_index_path = os.path.join(save_dir, "clip.index")
            clip_meta_path = os.path.join(save_dir, "clip_meta.json")

            faiss.write_index(clip_index, clip_index_path)
            with open(clip_meta_path, "w") as f:
                json.dump(clip_meta, f, indent=2)

        # -- Step 6: Save text RAG store for analysis text --
        combined_texts = [
            f"Image Description: {item['caption']}\n\nBody Analysis:\n{item['analysis']}"
            for item in image_analysis_texts
        ]

        metadatas = []
        for item in image_analysis_texts:
            md = dict(user_profile)
            md["caption"] = item["caption"]
            md["created_at"] = str(datetime.now())
            metadatas.append(md)

        profile_vectorstore = FAISS.from_texts(
            combined_texts, embedding=text_embeddings, metadatas=metadatas
        )
        profile_vs_path = os.path.join(save_dir, "fitness_profile_vectorstore")
        profile_vectorstore.save_local(profile_vs_path)

        # -- Step 7: Build combined analysis text (for display + compare) --
        analysis_text = "\n\n---\n\n".join([x["analysis"] for x in image_analysis_texts])

        # -- Step 8: Optional feedback match from uploaded docs --
        if document_vectorstore:
            docs = document_vectorstore.similarity_search(analysis_text, k=3)
            feedback = "\n\n".join([d.page_content for d in docs])
            analysis_text += f"\n\nüîç Based on your uploaded workout/diet, here's a feedback match:\n{feedback}"

        return {
            "error": None,
            "analysis_text": analysis_text,
            "items": image_analysis_texts,               # captions + analysis per image
            "user_profile": user_profile,

            # persistence for later chat/compare
            "profile_vectorstore_path": profile_vs_path,
            "clip_index_path": clip_index_path,
            "clip_meta_path": clip_meta_path,
        }

    except Exception as e:
        return {"error": f"‚ùå Error: {e}"}